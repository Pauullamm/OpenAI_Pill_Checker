{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO93ESZLAYFvyQbSUimn7nU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pauullamm/OpenAI_Pill_Checker/blob/main/OpenAI_Pill_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description: Python scripts to prepare a text-based AI model fine-tuned on an OpenAI davinci-002 model\n",
        "\n",
        "Have you ever wanted a pill identifer tool to check the name of a tablet/capsule by its description? This Jupyter notebook outlines steps to fine-tune an OpenAI model for this purpose, utilising pharmaceutical/manufacturer data from the Electronic Medicines Compendium"
      ],
      "metadata": {
        "id": "Z631m5Q8bZ5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai -q"
      ],
      "metadata": {
        "id": "dtIfzWf_oLaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Retrive url links of all medicines starting with a particular letter from the electronic medicines compendium"
      ],
      "metadata": {
        "id": "2ofEusx1cBz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "letter_B_url = 'https://www.medicines.org.uk/emc/browse-medicines/B'\n",
        "\n",
        "def get_elements_of_letter(url):\n",
        "  \"\"\"Gets the total number of drugs under the specific letter\n",
        "  Args:\n",
        "    url: url of the durgs of a specific letter\n",
        "  \"\"\"\n",
        "  r = requests.get(url)\n",
        "  letter_soup = BeautifulSoup(r.text, 'html.parser')\n",
        "  total_elements = letter_soup.find(class_='latest-updates-results-header-summary-total')\n",
        "  total_elements = total_elements.text.replace(\" \", \"\")\n",
        "  total_elements = int(total_elements.replace(\"resultsfound\", \"\"))\n",
        "  return total_elements\n",
        "\n",
        "def get_urls(num, link, show_progress=False,):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "      num (int): number of items on the page\n",
        "      show_progress: prints the item being processed to the screen, default value is False\n",
        "\n",
        "  Returns:\n",
        "      A set with the links for each item\n",
        "  \"\"\"\n",
        "  output_urls = set()\n",
        "  for i in tqdm(range(1, num + 1, 50)):\n",
        "\n",
        "    #iterate over over site number\n",
        "    url_to_check = f'{link}?offset={i}&limit=50'\n",
        "    response = requests.get(url_to_check)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    url_title_links = soup.find_all(class_=\"search-results-product-info-title-link emc-link\")\n",
        "    for j in url_title_links:\n",
        "      if \"ablet\" in j.text:\n",
        "        if show_progress:\n",
        "          print(f\"Processing: {j.text}\")\n",
        "        href = 'https://www.medicines.org.uk/' + j.get('href')\n",
        "        output_urls.add(href)\n",
        "      if \"apsule\" in j.text:\n",
        "        if show_progress:\n",
        "          print(f\"Processing: {j.text}\")\n",
        "        href_cap = 'https://www.medicines.org.uk/' + j.get('href')\n",
        "        output_urls.add(href_cap)\n",
        "\n",
        "  return output_urls\n",
        "\n",
        "\n",
        "B_urls = get_urls(num=get_elements_of_letter(letter_B_url), link=letter_B_url)\n"
      ],
      "metadata": {
        "id": "f34xz9YH0mA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Screen through each drug link starting with a particular letter to obtain drug description and manufacturer details"
      ],
      "metadata": {
        "id": "kkmxwOstcRd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gathering data from letter urls\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# nesting scraper in a single function for pharmaceutical form\n",
        "\n",
        "def find_drug_description(url):\n",
        "  discontinued = []\n",
        "  r = requests.get(url)\n",
        "  soup = BeautifulSoup(r.text, 'html.parser')\n",
        "  # getting name of medicines\n",
        "  title_tag = soup.find(id='PRODUCTINFO')\n",
        "  try:\n",
        "    title_parent = title_tag.parent\n",
        "    title = title_parent.find(class_='sectionWrapper').text\n",
        "  except Exception as e:\n",
        "    title = \"\"\n",
        "    print(f\"DISCONTINUED/NO SPC: {url}\")\n",
        "    discontinued.append(url)\n",
        "    pass\n",
        "\n",
        "  # getting description of medicine\n",
        "  tag = soup.find(id='FORM')\n",
        "  try:\n",
        "    desc_parent = tag.parent\n",
        "    all_desc = desc_parent.find_all(recursive=False)  # Restrict search within the parent div\n",
        "    dsc_output = \"\"\n",
        "    for desc in all_desc:\n",
        "        if desc != tag:\n",
        "          # Exclude the target element itself # Process the sibling element\n",
        "          dsc_output = desc.text + \" What is this drug?\"\n",
        "  except Exception as e:\n",
        "    dsc_output = \"\"\n",
        "    pass\n",
        "\n",
        "  # getting company name\n",
        "  try:\n",
        "    comp_name = soup.find(class_=\"product-header-company-name\").text\n",
        "  except Exception:\n",
        "    comp_name = \"\"\n",
        "    pass\n",
        "\n",
        "  return title.replace(\"\\n\", \"\"), dsc_output.replace(\"\\n\", \"\"), comp_name.replace(\"\\n\", \"\"), discontinued\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "output_dict = []\n",
        "for i in tqdm(B_urls):\n",
        "  name, description, company, disc_med = find_drug_description(i)\n",
        "  output_dict.append({\"Name\": name,\n",
        "               \"Description\": description,\n",
        "               \"Company\": company})\n",
        "\n",
        "df = pd.DataFrame(output_dict)\n",
        "print(df.to_string())\n",
        "df.to_csv('OSD(B).csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "C0mu649ar-fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Convert the collected data to JSON format"
      ],
      "metadata": {
        "id": "2cUb0thicg1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def df_to_training_data(df):\n",
        "  \"\"\"\n",
        "  Converts a pandas DataFrame to a list of dictionaries in training_data format.\n",
        "\n",
        "  Args:\n",
        "      df (pandas.DataFrame): The DataFrame to convert.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of dictionaries in training_data format.\n",
        "  \"\"\"\n",
        "  training_data = []\n",
        "  for index, row in df.iterrows():\n",
        "    # Merge last column into first column\n",
        "    completion = f\"{row[0]} {str(row[2]).strip()}\"\n",
        "    merged_prompt = f\"{row[1]}\"\n",
        "\n",
        "    # Create dictionary and append to training_data\n",
        "    data_dict = {\"prompt\": merged_prompt, \"completion\": completion}\n",
        "    training_data.append(data_dict)\n",
        "\n",
        "  return training_data\n",
        "\n",
        "df = pd.read_csv(\"OSD(B).csv\")\n",
        "training_data = df_to_training_data(df.copy())\n",
        "print(json.dumps(training_data, indent=2))\n"
      ],
      "metadata": {
        "id": "uSbewR9fi2yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Filter through data to prepare training dataset"
      ],
      "metadata": {
        "id": "EtWd6lgGcnMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "training_file_name = \"training_dataB.jsonl\"\n",
        "\n",
        "def prepare_data(dictionary_data, final_file_name):\n",
        "  with open(final_file_name, 'w') as outfile:\n",
        "\n",
        "    for entry in dictionary_data:\n",
        "      json.dump(entry, outfile)\n",
        "      outfile.write('\\n')\n",
        "\n",
        "def remove_nan_dicts(data):\n",
        "  \"\"\"\n",
        "  Removes dictionaries containing \"nan\" values from a JSON object or list.\n",
        "\n",
        "  Args:\n",
        "      data (object): The JSON data to process (dict or list).\n",
        "\n",
        "  Returns:\n",
        "      object: The modified JSON data with \"nan\" dictionaries removed.\n",
        "  \"\"\"\n",
        "  # Iterate through the list of dictionaries\n",
        "  output_json = []\n",
        "  for d in data:\n",
        "    if 'nan' in d.values():\n",
        "      print(d)\n",
        "      continue\n",
        "    output_json.append(d)\n",
        "\n",
        "  return output_json\n",
        "\n",
        "\n",
        "clean_data = remove_nan_dicts(training_data)\n",
        "prepare_data(clean_data, training_file_name)\n",
        "print(json.dumps(clean_data, indent=2))\n",
        "print(clean_data)\n"
      ],
      "metadata": {
        "id": "o8yGQZfGqvUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Upload data to OpenAI API fine-tuning"
      ],
      "metadata": {
        "id": "ewpa5v9ycuFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import openai\n",
        "\n",
        "api_key = \"YOUR OPENAI API KEY\"\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "training_file_id = client.files.create(\n",
        "  file=open(training_file_name, \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")"
      ],
      "metadata": {
        "id": "1t2TrY_fsX_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Initiate model fine-tuning"
      ],
      "metadata": {
        "id": "rP5x22uLc2YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.fine_tuning.jobs.create(\n",
        "  training_file=training_file_id.id,\n",
        "  model=\"davinci-002\",\n",
        "  hyperparameters={\n",
        "    \"n_epochs\": 5,\n",
        "\t\"batch_size\": 3,\n",
        "\t\"learning_rate_multiplier\": 0.3\n",
        "  }\n",
        ")\n",
        "job_id = response.id\n",
        "status = response.status\n",
        "\n",
        "print(f'Fine-tunning model with jobID: {job_id}.')\n",
        "print(f\"Training Response: {response}\")\n",
        "print(f\"Training Status: {status}\")"
      ],
      "metadata": {
        "id": "AxTbIhA4UlSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Use fine-tuned model using prompts to describe tablet or capsule details"
      ],
      "metadata": {
        "id": "iABGAEvBdBsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "result = client.fine_tuning.jobs.list()\n",
        "fine_tuned_model = result.data[0].fine_tuned_model\n",
        "\n",
        "new_prompt = 'Yellow coloured, scored tablets with a one-sided embossment „BIS 5\".What is this drug?'\n",
        "answer = client.completions.create(\n",
        "  model='ft:davinci-002:personal::92hKSsQ8',\n",
        "  prompt=new_prompt,\n",
        "  max_tokens=20\n",
        ")\n",
        "\n",
        "print(answer.choices[0].text)\n"
      ],
      "metadata": {
        "id": "VE1h7hjooKcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654d492f-f375-4183-8f2d-13a8a9c06af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bisoprolol Fumarate 5 mg Tablets Sandoz Limited ->->->->\n"
          ]
        }
      ]
    }
  ]
}